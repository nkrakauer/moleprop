{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "- import workflow.py and need to make sure that integration_helpers.py is under the same dir as workflow.py<br/>\n",
    "- change '../' to your dir of workflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepchem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eba756b7d67f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mworkflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/moleprop/workflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintegration_helpers\u001b[0m                               \u001b[0;31m# for removing duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepchem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mundo_transforms\u001b[0m  \u001b[0;31m# for getting real predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepchem'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')     \n",
    "import workflow as wf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "- Loading raw data(dataset with duplicates) from the path of \"data_dir + file_name\" <br/>\n",
    "- Change data_dir and file_name to the path and file you want to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"About to load\")\n",
    "loader = wf.Loader\n",
    "data = loader.load(file_name = 'integrated_dataset.csv',data_dir = '/srv/home/xsun256/Moleprop/summer19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "- Data cleaning (removing duplictaes) and splitting\n",
    "- Returns indices of training sets and test sets, and new dataset after removing duplicates\n",
    "- params:\n",
    "  - data: DataFrame\n",
    "  - n_splits (for k_fold splitter): n of n-fold cv\n",
    "  - test_group (for LOG splitter): name of the dataset you want to leave out as test group\n",
    "- Other params you can set:\n",
    "  - k_fold splitter:\n",
    "    - shuffle( = True in defualt): True or False\n",
    "  - LOG splitter: \n",
    "    - frac( = 1 in default): \n",
    "        - training set: all other datasets + (1-frac) * left-out dataset\n",
    "        - test set: frac* left-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 3\n",
    "# Data cleaning (removing duplictaes) and splitting\n",
    "print(\"About to split\")\n",
    "splitter = wf.Splitter\n",
    "indices,dataset = splitter.k_fold(data, n_splits = 3)\n",
    "# indices,dataset = splitter.LOG(data,'left-out group name')  # LOG splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "- conduct CV or LOG_validation\n",
    "- Param:\n",
    "    - dataset: DataFrame\n",
    "    - indices: indices returned from splitter\n",
    "    - model name: accept \"GC\",\"GraphConv\",\"graphconv\", and \"MPNN\"\n",
    "    - model_args (= None in default): \n",
    "        - GC args:\n",
    "            - nb_epoch, \n",
    "            - batch_size, \n",
    "            - n_tasks, \n",
    "            - graph_conv_layers, \n",
    "            - dense_layer_size, \n",
    "            - dropout, \n",
    "            - mode\n",
    "        - MPNN args:\n",
    "            - 'n_tasks':1,\n",
    "            - 'n_atom_feat':75,\n",
    "            - 'n_pair_feat':14,      # NEED to be 14 for WaveFeaturizer\n",
    "            - 'T':1,\n",
    "            - 'M':1,\n",
    "            - 'batch_size':32,\n",
    "            - 'nb_epoch': 50,\n",
    "            - 'learning_rate':0.0001,\n",
    "            - 'use_queue':False,\n",
    "            - 'mode':\"regression\"\n",
    "     - metrics: accept \"AAD\", \"RMSE\", \"MAE\", \"R2\"\n",
    "- Return:\n",
    "    - scores: a dictionart of scores (based on the merics you set):\n",
    "        - avg_AAD and AAD_list (list of AAD score from every iteration)\n",
    "        - avg_R2 and R2_list\n",
    "        - avg_RMSE and RMSE_list\n",
    "        - avg_MAE and MAE_list\n",
    "    - predictions: list of predictions from every iteration\n",
    "    - test_Dataset: list of test datasets from every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GC_args_example = {'nb_epoch': 80,\n",
    "        'batch_size': 50,\n",
    "        'n_tasks': 1,\n",
    "        'graph_conv_layers':[64,64],\n",
    "        'dense_layer_size': 256,\n",
    "        'dropout': 0.0,           # for testing if this workflow tool can correctly use default dropout if it is not inputted\n",
    "        'mode': 'regression'}\n",
    "'''\n",
    "print(\"About to simulate\")\n",
    "scores,predictions,test_datasets = wf.Run.cv(dataset,indices, 'GC',model_args = None,n_splits = 3, metrics = ['AAD', 'RMSE', 'MAE', 'R2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP5\n",
    "- Print out result and save parity plots and hitogram plots\n",
    "- Params:\n",
    "    - plot_name: name of plot without add '.png'\n",
    "    - text: **dictionary** of scores that you want to add to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in scores:\n",
    "    print(key+\" = \"+str(scores[key]))\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    p_name = \"./parity_plot/MPNN_parity_\"+str(i)\n",
    "    txt = {\"Iteration number \":i+1,\"RMSE\":scores['RMSE_list'][i], \"R2\":scores['R2_list'][i], \"MAE\":scores['MAE_list'][i], \"AAD\":scores['AAD_list'][i]}\n",
    "    wf.Plotter.parity_plot(predictions[i],test_datasets[i], plot_name = p_name, text = txt)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    r_name = \"./residual_plot/MPNN_residual_\"+str(i)\n",
    "    txt = {\"Iteration number \":i+1,\"RMSE\":scores['RMSE_list'][i], \"R2\":scores['R2_list'][i], \"MAE\":scores['MAE_list'][i], \"AAD\":scores['AAD_list'][i]}\n",
    "    wf.Plotter.residual_histogram(predictions[i],test_datasets[i], plot_name = r_name, text = txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
