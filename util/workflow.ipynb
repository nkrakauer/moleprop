{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# pkg needed for GraphConv\n",
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvModel\n",
    "\n",
    "# pkg needed for MPNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    data = pd.DataFrame()\n",
    "    def load(file_name, data_dir = './'):\n",
    "        \"\"\"\n",
    "        load data from .csv file\n",
    "        \"\"\"\n",
    "        # TODO: need to clean data before loading?\n",
    "        data_file = os.path.join(data_dir, file_name)\n",
    "        if not os.path.exists(data_file):\n",
    "            if data_dir == './':\n",
    "                data_dir = \"current\"\n",
    "            error_msg = file_name + \" was not found in \" + data_dir + \" directory\"\n",
    "            sys.exit(error_msg)\n",
    "        print(\"|||||||||||||||||||||Loading \" + file_name+ \"|||||||||||||||||||||||\")\n",
    "        data = pd.read_csv(data_file) # encoding='latin-1' might be needed\n",
    "        return data\n",
    "\n",
    "    def getinfo():\n",
    "        \"\"\"\n",
    "        get information of the dataset\n",
    "        \"\"\"\n",
    "        sources = data.source.unique()\n",
    "        source_info = dict()\n",
    "        for s in sources:\n",
    "            counter = 0\n",
    "            for i in range(0,len(data.index)):\n",
    "                if data.iloc[i]['source'] == s:\n",
    "                    counter += 1\n",
    "            source_info[s] = [counter]\n",
    "        print('-----------------------------------------------------')\n",
    "        print(\"Dataset length is: \", len(data.index))\n",
    "        print('-----------------------------------------------------')\n",
    "        print(\"Dataset sources info: \")\n",
    "        for s in sources:\n",
    "            print(\"  Source Name: \" + s + \", Number of Data: \" + str(source_info[s]))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "\n",
    "    def k_fold(dataset, n_splits = 3, shuffle = True):\n",
    "        \"\"\"\n",
    "        split data into k-fold\n",
    "        return indices of training and test sets\n",
    "        \"\"\"\n",
    "        if shuffle == True:\n",
    "            random_state = 4396\n",
    "        kf = KFold(n_splits, shuffle, random_state)\n",
    "        indices = kf.split(dataset)\n",
    "        return indices\n",
    "\n",
    "    def LOG(dataset, test_group):  # leave out group\n",
    "        \"\"\"\n",
    "        split dataset by leaving out a specific source as test set\n",
    "        \"\"\"\n",
    "        test_indices = []\n",
    "        train_indices = list(range(len(dataset.index)))\n",
    "        print(\"||||||||||||||||||| \"+test_group+ \" will be used as test set|||||||||||||||||||\")\n",
    "        for i in range(0,len(dataset.index)):\n",
    "            if dataset.iloc[i]['source'] == test_group:\n",
    "                test_indices.append(i)\n",
    "                train_indices.remove(i)\n",
    "        return (train_indices, test_indices)\n",
    "    \n",
    "    def leave_out_moleClass(dataset, mole_class_to_leave_out):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        leave out a specific mole class\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate:\n",
    "        \n",
    "    def cv(data,\n",
    "           indices, # k-fold indices of training and test sets\n",
    "           model,  # need to be either MPNN or GraphConv\n",
    "           model_args = None, \n",
    "           n_splits = 3):   \n",
    "        \"\"\"\n",
    "        pass data into models (MPNN or graphconv) and conduct cross validation\n",
    "        \"\"\"\n",
    "        if not (model == 'MPNN' or model == 'graphconv'):\n",
    "            sys.exit(\"Only support MPNN model and GraphConv model\")\n",
    "        cv_scores = []\n",
    "        for train_indices, test_indices in indices:\n",
    "            train_set = data.iloc[train_indices]\n",
    "            test_set = data.iloc[test_indices]\n",
    "            train_set.to_csv('train_set.csv',index = False)\n",
    "            test_set.to_csv('test_set.csv',index = False)\n",
    "            if model == 'MPNN':\n",
    "                score = Model.MPNN(model_args, \"train_set.csv\", \"test_set.csv\")\n",
    "            elif model == 'graphconv':\n",
    "                score = Model.graphconv(model_args,\"train_set.csv\", \"test_set.csv\")       \n",
    "            cv_scores.append(score)\n",
    "        avg_cv_score = sum(cv_scores)/n_splits\n",
    "        return avg_cv_score\n",
    "\n",
    "    def LOG_validation(data,\n",
    "                       indices, \n",
    "                       model, \n",
    "                       model_args = None):\n",
    "        \"\"\"\n",
    "        Conduct leave-out-group validation\n",
    "        \"\"\"\n",
    "        \n",
    "        if not (model == 'MPNN' or model == 'graphconv'):\n",
    "            sys.exit(\"Only supports MPNN model and graphconv model\")\n",
    "        train_indices, test_indices = indices\n",
    "        train_set = data.iloc[train_indices]\n",
    "        test_set = data.iloc[test_indices]\n",
    "        train_set.to_csv('train_set.csv',index = False)\n",
    "        test_set.to_csv('test_set.csv',index = False)\n",
    "        if model == 'MPNN':\n",
    "            score = Model.MPNN(model_args, \"train_set.csv\", \"test_set.csv\")\n",
    "        elif model == 'GraphConv':\n",
    "            score = Model.graphconv(model_args, \"train_set.csv\", \"test_set.csv\")       \n",
    "        return score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    default_args = {\n",
    "        'graphconv': {\n",
    "            'nb_epoch': 50, \n",
    "            'batch_size': 64, \n",
    "            'n_tasks': 1, \n",
    "            'graph_conv_layers':[64,64],\n",
    "            'dense_layer_size': 256,\n",
    "            'dropout': 0,\n",
    "            'mode': 'regression'},\n",
    "        'MPNN':{\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def graphconv(args, train_set, test_set):\n",
    "        # parse arguments\n",
    "        if args == None:\n",
    "            args = Model.default_args['graphconv']\n",
    "        nb_epoch = args[\"nb_epoch\"]\n",
    "        batch_size =  args[\"batch_size\"]\n",
    "        n_tasks = args[\"n_tasks\"]\n",
    "        graph_conv_layers = args[\"graph_conv_layers\"]\n",
    "        dense_layer_size = args[\"dense_layer_size\"]\n",
    "        dropout = args[\"dropout\"]\n",
    "        mode = args[\"mode\"]  # regression or classificiation\n",
    "\n",
    "        flashpoint_tasks = ['flashPoint']  # Need to set the column name to be excatly \"flashPoint\"\n",
    "        loader = dc.data.CSVLoader(tasks = flashpoint_tasks, \n",
    "                                        smiles_field=\"smiles\", \n",
    "                                        featurizer = dc.feat.ConvMolFeaturizer())\n",
    "        train_dataset = loader.featurize(train_set, shard_size=8192)\n",
    "        test_dataset = loader.featurize(test_set, shard_size=8192)\n",
    "        transformers = [\n",
    "            dc.trans.NormalizationTransformer(\n",
    "            transform_y=True, dataset=train_dataset, move_mean=True) # sxy: move_mean may need to change (3/23/2019)\n",
    "        ]\n",
    "        for transformer in transformers:\n",
    "            train_dataset = transformer.transform(train_dataset)\n",
    "        transformers = [\n",
    "            dc.trans.NormalizationTransformer(\n",
    "            transform_y=True, dataset=test_dataset, move_mean=True) # sxy: move_mean may need to change (3/23/2019)\n",
    "        ]\n",
    "        for transformer in transformers:\n",
    "             test_dataset = transformer.transform(test_dataset)\n",
    "        model = dc.models.GraphConvModel(n_tasks = n_tasks, mode = mode, dropout = dropout)\n",
    "        metric = dc.metrics.Metric(dc.metrics.rms_score, np.mean)\n",
    "        model.fit(train_dataset, batch_size = batch_size, nb_epoch = nb_epoch) \n",
    "        score = list( model.evaluate(test_dataset, [metric],transformers).values()).pop()\n",
    "        print(\"score is :\\n------------------------------\")\n",
    "        print(score)\n",
    "        print(\"--------------------------------\")\n",
    "        return score\n",
    "\n",
    "    def MPNN(args, train_set, test_set):\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
